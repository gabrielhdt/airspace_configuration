\documentclass{article}
\usepackage{fontspec}
\usepackage{polyglossia}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{unicode-math}
\usepackage{amsmath,amsthm,stmaryrd}
\usepackage{algorithm,algpseudocode}

\title{Airspace configuration with Monte Carlo tree search}
\author{Us}
\date{\today}


\begin{document}
\maketitle

\section{Preliminaries}
\subsection{Base theory: the environment and notations}
Notations based on Markov Decision Problem's theory (taken
from~\cite{browne2012survey})
\begin{itemize}
  \item \(\mathcal{S}\) a set of states,
  \item \(\mathcal{A}\) a set of actions,
  \item \(T(s, a, s')\) a transition model that determines the probability of
      reaching state \(s'\) if action \(a\) is applied to state \(s\),
  \item \(R(s)\) a reward function
\end{itemize}

A policy is a mapping \(\pi \colon \mathcal{S} \to \mathcal{A}\). The aim is to
output, from a state \(s\), the action that gives the highest reward.

\subsection{Monte carlo methods~\cite{browne2012survey}}
We denote by \(Q(s, a)\) the expected reward of an action. Let
\begin{itemize}
  \item \(N(s, a)\) be
    the number of times action \(a\) has been selected from state \(s\),
  \item \(N(s)\) the number of times the game has been played from state \(s\),
  \item \(\mathbb{I}_i(s, a)\) is 1 if action \(a\) was selected from state
    \(s\) on the \(i\)th playout from state \(s\) else 0 and
  \item \(z_i\) is the result of the \(i\)th simulation played.
\end{itemize}
In Monte Carlo methods, we have
\begin{equation}
  Q(s, a) = \frac{1}{N(s, a)}\sum_{i = 1}^{N(s)}\mathbb{I}_i(s, a)z_i
\end{equation}

\subsection{Bandits methods}

\subsection{Monte Carlo tree search}
In the MCTS, the action \(a\) is the path that leads from the root to the best
node computed so far. To each node should correspond a state \(s\). This way, an
action \(a\) from a node with state \(s\) gives an other node with an other
state \(s'\).

\bibliography{article}
\bibliographystyle{plain}

\section{Algorithms}
\subsection{Monte carlo}
\begin{algorithm}
  \caption{General MCTS approach~\cite{browne2012survey}}
  \begin{algorithmic}
    \Function{MctsSearch}{$s_0$}
    \State{}create root node \(v_0\) with state \(s_0\)
    \While{within computational budget}
    \State{} \(v_l \gets\) \Call{TreePolicy}{$v_0$}
    \State{} \(\Delta \gets\) \Call{DefaultPolicy}{$s(v_l)$}
    \State{} \Call{Backup}{$v_l$, $\Delta$}
    \EndWhile{}
    \Return{\(a(\)\Call{BestChild}{$v_0$}\()\)}
    \EndFunction{}
  \end{algorithmic}
\end{algorithm}
\end{document}
