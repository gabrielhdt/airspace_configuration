\documentclass[oneside,twocolumn]{article}

\usepackage{fontspec}
\usepackage{polyglossia}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{unicode-math}
\usepackage{amsmath,amsthm,stmaryrd}
\usepackage{algorithm,algpseudocode}
\usepackage{paralist}

\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\first}{first}
\DeclareMathOperator{\state}{state}


%\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry}
%\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption}
%\usepackage{booktabs}
%
%\usepackage{lettrine}
%
%\usepackage{enumitem}
%\setlist[itemize]{noitemsep}
%
%\usepackage{abstract}
%% Set the "Abstract" text to bold
%\renewcommand{\abstractnamefont}{\normalfont\bfseries}
%% Set the abstract itself to small italic text
%\renewcommand{\abstracttextfont}{\normalfont\small\itshape}
%
%% Allows customization of titles
%\usepackage{titlesec}
%
%% Headers and footers
%\usepackage{fancyhdr}
%% All pages have headers and footers
%\pagestyle{fancy}
%% Blank out the default header
%% \fancyhead{}
%% Blank out the default footer
%\fancyfoot{}
%% Custom header text
%% \fancyhead[C]{Running title $\bullet$ May 2016 $\bullet$ Vol. XXI, No. 1}
%% Custom footer text
%\fancyfoot[RO,LE]{\thepage}
%\usepackage{titling} % Customizing the title section

\title{Airspace configuration decision based on Monte-Carlo tree search}
\author{Gabriel~Hondet, Beno\^{\i}t Viry}
\date{\today}
\pagestyle{headings}

%-----------------------------------------------------------------------------
%	TITLE SECTION
%-----------------------------------------------------------------------------

%% Move the title up
%\setlength{\droptitle}{-4\baselineskip}
%% Article title formatting
%\pretitle{\begin{center}\Huge\bfseries}
%% Article title closing formatting
%  \posttitle{\end{center}}

%\renewcommand{\maketitlehookd}{%
%  \begin{abstract}
%
%  \end{abstract}
%}

%-----------------------------------------------------------------------------
\begin{document}

\maketitle

\begin{abstract}
  A nice abstract
\end{abstract}


\section*{Notations}
\begin{compactitem}
  \item $m$: elementary module;
  \item $S$: sector control position, group of modules;
  \item $P$: partition of the airspace, group of sectors;
  \item $t$: current time;
  \item $C(P, t)$: cost of a given partition $P$ at time $t$;
  \item \(u, v \in \mathcal{N}^2\): nodes
  \item \(\mathcal{N}\): the set of nodes;
  \item $h \colon \mathcal{N} \to \mathbb{R}^+$: heuristic estimating the cost
    from a node \(u\) to a final node;
  \item \(\mu_v\): mean value of outcomes of simulations run through or from a
    node \(v\);
  \item \(T_v\): number of simulations run through or from a node \(v\);
  \item \(\first(\ell)\): returns the first element of \(\ell\).
\end{compactitem}

\section{Introduction}
The airspace is divided into sectors, themselves divided into elementary
modules. Each sector is managed by a controller working position composed of two
controllers.
During the day, sectors are split and merged to be able to
manage the varying traffic. Splitting creates smaller sectors and is therefore
used when traffic gets too dense. On the opposite, merging sectors allows fewer
controllers to manage the same airspace, and is therefore used when traffic
becomes sparse.

Currently, configuration is mainly decided on the fly by chief officer (TODO).
This decision is based on the actual workload on each position. In this
approach, future workload estimation is based on the controller's feelings and
it therefore lacks a workload prediction tool.
This papers aims at providing this tool as well as a method offering a smoother
workload during the day.

Several methods have been considered to solve the dynamic airspace configuration
problem, for instance via genetic algorithms in~\cite{sergeeva2017dynamic},
constraint local search in~\cite{jagare2013airspace}, integer linear
programming in~\cite{treimuth2016branch} or dynamic programming
in~\cite{bloem2010dynamic}.

In this paper, a temporal sequence of configuration is provided, as done
in~\cite{treimuth2016branch} or~\cite{sergeeva2017dynamic}.
Two costs are considered to create the sequence, namely the cost
associated to each configuration and a transition cost. The former is based on
the workload estimated for one sector given a neural network
(see~\cite{gianazza2010forecasting}).

The sequence of configurations is represented as a tree. The resulting
sequence will therefore be a path from the root to a leaf of this tree. This
problem is highly combinatorial (partitioning of the airspace). Since stochastic
tree search algorithms, combined with deep neural networks have proved
themselves worthy by outranking the best human player of one of the most highly
combinatorial game which is the game of Go, the Monte Carlo tree
search~\cite{browne2012survey} algorithm is used in this paper to fullfill the
previously mentioned task.



\section{Previous related works}\label{sec:previous_related_works}

The dynamic airspace configuration problem requires a model of the airspace. As
done in~\cite{sergeeva2017dynamic} or~\cite{treimuth2016branch} the airspace
is modelled via a graph. In those graphs, vertices represent elementary modules
and an edge links two adjacent modules. In~\cite{treimuth2016branch}, to be
able to produce a sequence of configurations, the graph is time dependent.
An other way to model the problem is to use a constrained set of
configurations as in~\cite{gianazza2010forecasting} and~\cite{bloem2010dynamic}.
This way any configuration will match specified requirements, which can be
qualified hard constraints.

In most cases the cost of a configuration is based on the workload. Each
approach seems to give their own representation of the workload. For instance,
~\cite{bedouet2016towards} determined workload density proportionally to the time
spent by aircraft in each sector. A simpler version~\cite{sergeeva2017dynamic}
only uses the number of aircraft. On the other hand, more complex methods,
involving many more inputs are also available. For
instance,~\cite{gianazza2010forecasting} used several indicators, such as sector
volume, or vertical incoming flows in the next 15 and 60 minutes. Those
indicators ease the training of a neural network forecasting the workload.

Other soft constraints appear to be relevant to have a better model of
the problem. For instance in~\cite{sergeeva2017dynamic}
and~\cite{bedouet2016towards} a
coordination cost is defined. It represents the surplus of work added by
flights travelling from one sector to an other. The shape of the
resulting sector is considered, as the simpler is
the shape, the easier it is to manage. Complex shapes are therefore avoided,
thanks to the notion of compactness in~\cite{jagare2013airspace} and balconies
in~\cite{sergeeva2017dynamic}. To smooth the transition between two
configurations, the work associated with the reallocation of one or more modules
is evaluated. This can be used in a cost function aimed to be minimised
(in~\cite{bedouet2016towards}).

\section{Algorithm}

The task of building an optimal sequence of partitions (lowering
as much as possible the workload of each controller) is fulfilled by a
stochastic tree search method, namely the Monte Carlo tree search. To assess the
exactness of the results, an exact method (here A\(^*\)) is used.

\subsection{Monte Carlo tree search}
\subsubsection{Overview}
Monte-Carlo is a best-first search method using stochastic simulations. The
algorithm actually uses two trees, an underlying tree associated to the model
(e.g.\ a game tree) and a search tree. The latter is built incrementally by the
algorithm the following way, at each step, starting from the root, the most
promising nodes towards the bottom of the tree are selected. Once a leaf of the
search tree is reached, a new node coming from the model tree is added to the
leaf. A simulation is then run from this newly added node to evaluate the
outcome of a path starting from it. The result of the simulation is eventually
backpropagated to the consecutive parents of the new node.

The above mentioned steps are denoted
\begin{compactenum}
\item selection (or tree walk): choosing successively most promising nodes,
\item expansion: adding new nodes to the search tree,
\item simulation (or random walk): choosing successively nodes from the model
  tree,
\item backpropagation (or backup): applying the result of the simulation to
  the search tree.
\end{compactenum}
Those phases are repeated until a stopping criterium (e.g.\ memory or time) is
reached, resulting in algorithm~\ref{alg:gen_mcts} where the tree policy
aggregates phases 1 and 2 to create a new node of the search tree and the
default policy gives an evaluation of the newly added node.
\begin{algorithm}
  \caption{General MCTS~\cite{browne2012survey}}\label{alg:gen_mcts}
  \begin{algorithmic}
    \Procedure{MctsSearchTree}{$v_0$}
    \While{within computational budget}
    \State{}\(\pi \gets\) \Call{TreePolicy}{$v_0$}
    \State{}\(v \gets \first(\pi)\)
    \State{}\(\Delta \gets\) \Call{DefaultPolicy}{$v$}
    \State{}\Call{Backup}{$\pi$, $\Delta$}
    \EndWhile{}
    \EndProcedure{}
  \end{algorithmic}
\end{algorithm}

\subsubsection{Extending the search tree}
\paragraph{Selection (algorithm~\ref{alg:tree_pol})}
The aim of the selection is to choose the most promising node among the children
available\footnote{i.e.\ possibly leading to the best evaluation} according to
a \emph{tree policy}. This type of problem can be solved by bandits methods.
Those methods consist in, given a bandit in front of several slot machines
(multi armed bandit), decide which machine will bring the highest reward knowing
the past result. The objective of bandits methods is thus to maximise the reward
and minimise the regret of not playing the best machine.

The bandit problem has been applied to MCTS via the Upper Confidence Tree
algorithm in~\cite{kocsis2006bandit} using the UCB1 equation~\ref{eq:ucb1}.
Let \(u\) be the node from which a child \(v\) must be selected to go deeper in
the tree, \(T_u\) the number of simulations carried out from node \(u\)
(which includes any simulation from nodes in any subtree of \(u\)) and
\(\beta\) a constant. The selected child \(v\) is thus the one maximising the
\emph{UCB1} equation
\begin{equation}
  \label{eq:ucb1}
  \mu_v + 2 \beta \sqrt{\frac{2 \log T_u}{T_v}}
\end{equation}

While the previous equation is well suited for two players games, it can be
tweaked to improve its efficiency in one player games or sequencing problems. An
alternative using the standard deviation of the outcome of the simulations is
proposed in~\cite{sebag2010fuse} called the \emph{UCB1-tuned} equation
\begin{equation}
  \mu_v +
  \sqrt{%
    \frac{\beta\log T_u}{T_v}
    \min\left( \frac{1}{4}, \sigma^2_v +
    \sqrt{%
      \frac{2 \log T_u}{T_v}
    }\right)
  }
\end{equation}
\begin{algorithm}
  \caption{UCT algorithm}\label{alg:tree_pol}
  \begin{algorithmic}
    \Function{TreePolicy}{$\pi$}
    \State{}\(v \gets \first(\pi)\)
    \If{$v$ is terminal}
    \State{}\Return{$\pi$}
    \Else{}
    \If{$v$ is not fully expanded}
    \State{}\Return{\Call{Expand}{$v$} $\cup \pi$}
    \Else{}
    \State{}$f \gets$ \Call{BestChild}{$v$}
    \State{}\Return{%
      \Call{TreePolicy}{$f \cup \pi$}
    }
    \EndIf{}
    \EndIf{}
    \EndFunction{}
  \end{algorithmic}
  \begin{algorithmic}
    \Function{BestChild}{$v$}
    \State{}\Return{$\argmax\{\Call{UCB}{v'} | v' \text{\,children of\,} v \}$}
    \EndFunction{}
  \end{algorithmic}
\end{algorithm}

\subparagraph{Exploration exploitation tradeoff} The constant \(\beta\) answers
to the exploration v exploitation dilemma. In the UCB1 equation~\ref{eq:ucb1},
the left hand term increases the value of less explorated nodes to consider them
as still promising. A high \(\beta\) value will therefore make the algorithm
prone to try unvisited nodes while a low value will consider almost exclusively
the results obtained so far, even if other paths are better.

\paragraph{Expansion (algorithm~\ref{alg:expansion})}
The above step is carried out only if, considering a node \(v\), a simulation
has been run through each child available from \(v\). In the case where one or
more children haven't been explored yet, one of the unexplored children is
chosen and a simulation is started from it.
\begin{algorithm}
  \caption{Expansion}\label{alg:expansion}
  \begin{algorithmic}
    \Function{Expand}{$v$}
    \State{}$\ell \gets \{ v | v \text{\,children of\,} u, T_v = 0 \}$
    \State{}\Return{random element from $\ell$}
    \EndFunction{}
  \end{algorithmic}
\end{algorithm}

\subsubsection{Simulation and backpropagation}
\paragraph{Simulation}
Once a node has been expanded, random nodes are chosen successively until a
terminal state is found. The cost of the resulting path, from the root to the
node is then evaluated and backpropagated to all the ancestors of the expanded
node.

\subparagraph{Heuristic} One might want to bias the randomness while choosing
nodes. This would imply using a heuristic which has to be able to discriminate
a node between its siblings.

\paragraph{Backpropagation}
The backpropagation consists in updating the values required to carry out the
tree policy. The values must be updated incrementally since the backpropagation
function has the current value of the parameters and the result of the
simulation as parameters. Thus, for UCB1 equation, the expected reward (mean)
and the total count are updated this way
\begin{algorithm}
  \caption{UCB1 backpropagation}
  \begin{algorithmic}
    \Procedure{Backpropagate}{$u$, $r$}
    \State{}\(\delta \gets r - \mu_u\)
    \State{}\(\mu_u \gets \mu_u + \frac{\delta}{T_u + 1}\)
    \State{}\(T_u \gets T_u + 1\)
    \EndProcedure{}
  \end{algorithmic}
\end{algorithm}

For the UCB1-tuned, the standard deviation has to be computed. It results in
algorithm~\ref{alg:backpropagate-tuned} which introduces the value \(m_2(u)\)
associated to a node \(u\) to compute the standard deviation via the formula
\begin{equation}
  \sigma^2_u = \frac{m_2(u)}{T_u}.
\end{equation}
\begin{algorithm}
  \caption{UCB1-tuned backpropagation}\label{alg:backpropagate-tuned}
  \begin{algorithmic}
    \Procedure{Backpropagate}{$u, r$}
    \State{}\(\delta \gets r - \mu_u\)
    \State{}\(\mu_u \gets \mu_u + \frac{\delta}{T_u + 1}\)
    \State{}\(\delta' \gets r - \mu_u\)
    \State{}\(m_2(u) \gets m_2(u) + \delta \delta'\)
    \State{}\(T_u \gets T_u + 1\)
    \EndProcedure{}
  \end{algorithmic}
\end{algorithm}


\subsection{Sequence building}
\subsubsection{Node selection}\label{sssec:node_selection}
The building of the final sequence of nodes differs from the selection on the
aim, the selection chooses the most promising node while the final sequence
must be composed of the best. This means that the final path build won't use the
UCB equation. Chaslot \textit{et al.}~\cite{browne2012survey} propose several
methods to select nodes,
\begin{compactitem}
  \item max-child: select the child with highest reward;
  \item robust child: select the most visited root child;
  \item secure child: select the child maximising a lower confidence bound.
\end{compactitem}

\subsubsection{Iterative pathfinding}
The path is built iteratively by calling successive Monte Carlo tree searches.
Say an MCTS has been called on a node \(u\). Once a stopping criterium is
matched, a node \(v\) among the ones reachable from \(u\) is selected via one of
the previously mentioned policies. Then an mcts is called back with node \(u\)
as the root node. The path is composed of the successive roots. The algorithm is
described at~\ref{alg:path_building}.
\begin{algorithm}
  \caption{%
    Path building. The {\sc NextNode} (here max-child) function is one among
    those in~\ref{sssec:node_selection}.
  }\label{alg:path_building}
  \begin{algorithmic}
    \Function{MctsSearch}{$u_0, n$}
    \State{}\(u \gets u_0\)
    \State{}\(\pi \gets \{u\}\)
    \For{$i = 1$ to $n$}
      \State{}\Call{MctsSearchTree}{$u$}
      \State{}\(u \gets\)\Call{NextNode}{$u$}
      \State{}\(\pi \gets \pi \cup \{u\}\)
    \EndFor{}
    \State{}\Return{$\pi$}
    \EndFunction{}
    \Function{NextNode}{$u$}
    \State{}\Return{$\argmax\{\mu_v | v \text{\,children of\,} u\}$}
    \EndFunction{}
  \end{algorithmic}
\end{algorithm}
\subsection{A\(^*\)}
To evaluate the exactness of the paths given by the MCTS algoithm, the A\(^*\)
algorithm is used. A\(^*\) is an exact depth first search pathfinding algorithm
which uses a heuristic function to guide its search. The algorithm is given
in~\ref{alg:astar} where \(u_0\) is the initial state, \(T\) the set of termial
nodes, \(h\) a heuristic function estimating the cost from a state \(u\) given
as argument to a final state, \(k \colon \mathcal{N}^2 \to \mathbb{R}\). The
function \(\first(G)\) returns the first element of \(G\) and
\(f\mathup{-insert}(G, v)\) inserts \(v\) in \(G\) ordering first by \(f\)
increasing then by \(g\) decreasing.
\begin{algorithm}
  \caption{A\(^*\) algorithm~\cite{alliotschiex2002ia&it}}\label{alg:astar}
  \begin{algorithmic}
    \Procedure{A$^*$}{$u_0$}
    \State{}\(G \gets u_0; D \gets \emptyset; g(u_0) \gets 0; f(u_0) \gets 0\)
    \State{}\(\mathup{father}(u_0) \gets \emptyset\)
    \While{$G \neq \emptyset$}
    \State{}\(u \gets \first(G); G \gets G \backslash \{u\}\)
    \State{}\(D \gets D \cup \{u\}\)
    \If{$u \in T$}
    \State{}\Return{father}
    \EndIf{}
    \For{$v$ in childen of $u$}
    \If{$v \notin D \cup G$ or $[g(v) > g(u) + k(u, v)]$}
    \State{}\(g(v) \gets g(u) + k(u, v)\)
    \State{}\(f(v) \gets g(v) + h(v)\)
    \State{}\(\mathup{father}(v) \gets u\)
    \State{}\(f\mathup{-insert}(G, v)\)
    \EndIf{}
    \EndFor{}
    \EndWhile{}
    \EndProcedure{}
  \end{algorithmic}
\end{algorithm}

\paragraph{Heuristic}
To be sure to have the optimal solution, the heuristic \(h\) has to be
\emph{minimal} i.e.\ let \(h^*\) be the optimal heuristic (the one giving the
true distance from a node to a final state), then for any node \(u\), \(h(u)
\leq h^*(u)\).


\section{Model}

In this section, we discuss the model established to approach this configuration
problem. First we defined a structure for a controled sector and all possible
transitions through time. Then a workload model is defined. And finally we
determined a cost per partition and a cost function aimed to be minimized.

\subsection{State}



\subsection{workload}

$p_{low}$
$p_{normal}$
$p_{high}$

\subsection{Partition cost and cost function}

\subsubsection{Partition cost}

In order to evaluate a given partition $P$ at time $t$, a cost $C(P, t)$ needs
to be defined. This cost depend on the workload in each the sector. We give the
following definitions~\cite{ferrari2017} to represent a high
(respectively normal and low) cost for the partition:
\begin{itemize}
  \item $c_+(P, t) = \sum_{S \in P_t} \delta_{h}(S, t) p_{high}^{S, t} |S|^2$
  \item $c_=(P, t) = 1 / \left(
      \sum_{S \in P_t} \delta_{n}(S, t) p_{norm}^{S} |S|^{-2}
    \right)$
  \item $c_-(P, t) = \sum_{S \in P_t} \delta_{l}(S, t) p_{low}^{S, t} |S|^{-2}$
\end{itemize}
with $\delta_{h}(P, t)$ (resp. $\delta_{n}(P_t)$ and $\delta_{l}(P_t)$) equal 1
if the probability $p_{high}$ (resp. $p_{normal}$ and $p_{low}$) is superior to
the two others, and 0 otherwise.

It is now possible to assign a cost to each partition. This cost is linear
regarding $c_+$, $c_=$, $c_-$ and $n$ (cardinal of the partition $P$). The
partition cost is defined as follows:
\begin{equation}
  C(P, t) = \alpha c_+ + \beta c_= + \gamma c_- +\lambda n
\end{equation}

The parameters $\alpha$, $\beta$, $\gamma$ and $\lambda$ (all positive)
determine a priority on which parameter to optimize. For instance, a high
value $\alpha$ help to
minimize $c_+$, hence the overall number of sectors with too much tafic.
In a real application, it maybe interesting to order those parmaeters as
follow: $\alpha > \gamma > \beta > \lambda$.

\subsubsection{Transition cost}

In an operational context, each reconfiguration raise the workload for
controllers. This trend needs to be considered and a transition cost needs to
be defined for this matter. A transition is (TODO), and the transition cost is
then:
\begin{equation}
  C_{tr}(P_1, P_2) =
  \begin{cases}
    0 & \text{if\,} P_1 = P_2\\
    1 & \text{otherwise}
  \end{cases}
\end{equation}

\subsubsection{Objective function}

With a cost defined for partition and transition, it is now possible to
aggregate everything in order to build a cost function over an entire
path. For a path $\pi = [P_0, \dots, P_n]$ with a time from $t_0$ to $t_n$,
the cost function is given by:
\begin{equation}
  \begin{split}
    f(\pi) = C(P_0, t_0) + \sum_{i = 1}^{n} [C(P_i, t_i) +\\
    \theta C_{tr}(P_{i-1}, P_i)]
  \end{split}
\end{equation}

with $\theta > 0$ a parameter to determine.
This is the immediate function to minimize in the A$^*$ algorithm.

The Monte Carlo Tree search algorithm maximize a objective function. This function
represent a potential reward over a path, i.e.\ a list of partitions over time.
Given the loss function defined previously, the target function can be
construct as:
\begin{equation}
  \psi(\pi) = \frac{1}{f(\pi)}
\end{equation}


\subsection{Heuristic}
To use effectively the A\(^*\) algorithm, a heuristic is needed. Using the null
heuristic (\(\forall u \in \mathcal{N}, h(u) = 0\)) revealed to be inefficient
considering the branching factor. To build a minimal heuristic, the limitations
introduced in~\ref{ssec:partitions} are not considered, neither those associated
to the previous partition (split, merge or transfer). This allows to generate
the sequence of the best partitions among all possible at each time step.

\bibliography{article}
\bibliographystyle{plain}
\end{document}
